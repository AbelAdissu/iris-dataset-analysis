{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOr6C49SRCs2s0EmyGp5Fc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbelAdissu/iris-dataset-analysis/blob/main/Iris_Dataset_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Loading Data and Data Preprocessing"
      ],
      "metadata": {
        "id": "p231Gpsu_6l5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IedOzXPW_5p5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "data['target'] = iris.target\n",
        "\n",
        "# Split the dataset into features (X) and the target variable (y)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform feature scaling (standardization) on the training and testing data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Encode the target variable (if it's not already encoded)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation 1:\n",
        "\n",
        "In this cell, we import the required libraries and load the Iris dataset.\n",
        "We create a DataFrame from the dataset, separate features (X) and the target variable (y).\n",
        "The dataset is split into training and testing sets using train_test_split.\n",
        "Feature scaling is performed with standardization.\n",
        "The target variable is encoded using LabelEncoder.\n",
        "Please note that data preprocessing is an essential step in machine learning to ensure that the data is in a suitable format for modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "wxJRgZltAZjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: K-Nearest Neighbors (KNN) and Cross-Validation"
      ],
      "metadata": {
        "id": "4lenqFsqBZVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "k_range = list(range(1, 32))\n",
        "param_grid = dict(n_neighbors=k_range, weights=[\"uniform\", \"distance\"])\n",
        "knn = KNeighborsClassifier()\n",
        "grid = GridSearchCV(knn, param_grid, cv=10, scoring=\"accuracy\")\n",
        "grid.fit(X, y)\n"
      ],
      "metadata": {
        "id": "bpsbNkQqAIN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation 2:\n",
        "\n",
        "In this cell, we import the KNeighborsClassifier and relevant modules for model evaluation.\n",
        "We define a range of k values (number of neighbors) and create a parameter grid including weight options (uniform and distance).\n",
        "A KNN classifier is instantiated for grid searching.\n",
        "GridSearchCV is used to perform hyperparameter tuning with 10-fold cross-validation.\n"
      ],
      "metadata": {
        "id": "Amvyv4T1BCGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: KNN Model Testing"
      ],
      "metadata": {
        "id": "YhQRfL6jBfE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\n",
        "knn.fit(X, y)\n",
        "knn.predict([[3, 5, 4, 2]])\n"
      ],
      "metadata": {
        "id": "aH4JBQ9JAj3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation 3:\n",
        "\n",
        "In this cell, we create a KNN classifier with n_neighbors=1 and weights=\"uniform\".\n",
        "The model is trained on the entire dataset.\n",
        "We make a prediction for a sample input [3, 5, 4, 2] to demonstrate how the model works.\n",
        "This section is for understanding the model behavior with specific hyperparameters and data. It's essential for educational purposes to see the model in action."
      ],
      "metadata": {
        "id": "6WUotkMkBQMU"
      }
    }
  ]
}